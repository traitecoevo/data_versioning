%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Please note that whilst this template provides a 
% preview of the typeset manuscript for submission, it 
% will not necessarily be the final publication layout.
%
% letterpaper/a4paper: US/UK paper size toggle
% num-refs/alpha-refs: numeric/author-year citation and bibliography toggle

%\documentclass[letterpaper]{oup-contemporary}
\documentclass[a4paper,num-refs]{assets/oup-contemporary}

%%% Journal toggle; only specific options recognised.
%%% (Only "gigascience" and "general" are implemented now. Support for other journals is planned.)
\journal{gigascience}

\usepackage{graphicx}
\usepackage{siunitx}

%%% Flushend: You can add this package to automatically balance the final page, but if things go awry (e.g. section contents appearing out-of-order or entire blocks or paragraphs are coloured), remove it!
% \usepackage{flushend}

\title{Datastorr: a workflow and package for delivering successive versions of ``evolving data'' directly into R}

%%% Use the \authfn to add symbols for additional footnotes, if any. 1 is reserved for correspondence emails; then continuing with 2 etc for contributions.
\author[1,\authfn{1}]{Daniel S. Falster}
\author[2]{Richard G. FitzJohn}
\author[3]{Matthew  W. Pennell}
\author[1]{William K. Cornwell}

\affil[1]{Evolution \& Ecology Research Centre, and School of Biological, Earth and Environmental Sciences,
University of New South Wales, Sydney NSW 2052, Australia}
\affil[2]{Department of Infectious Disease Epidemiology, Imperial College London, Faculty of Medicine, Norfolk Place, London W2 1PG, United Kingdom}
\affil[3]{Department of Zoology and Biodiversity Research Centre, University of British Columbia, Vancouver B.C. V6T 1Z4, Canada}

%%% Author Notes
\authnote{\authfn{1}daniel.falster@unsw.edu.au}

%%% Paper category
\papercat{Technical Note}

%%% "Short" author for running page header
\runningauthor{Falster et al.}

%%% Should only be set by an editor
\jvolume{00}
\jnumber{0}
\jyear{2018}

\begin{document}

\begin{frontmatter}
\maketitle
\begin{abstract}
The sharing and re-use of data has become a cornerstone of modern science. Multiple platforms now allow quick and easy data sharing. So far, however, data publishing offers limited functions for interacting with evolving datasets - those that continue to grow with time as more records are added, errors fixed, and new data structures are created. In this article, we describe a workflow for maintaining and distributing successive versions of an evolving dataset, allowing users to retrieve and load different versions directly into the R platform. Our workflow utilises tools and platforms used for development and distribution of successive versions of a open source software, including version control, GitHub and semantic versioning, and applies these to the analogous process of developing successive versions of an open source dataset. Moreover, we argue that this model allows for individual research groups to achieve a dynamic and versioned model of data delivery at no cost. 
\end{abstract}

\begin{keywords}
Data sharing; Version control; Semantic versioning
\end{keywords}
\end{frontmatter}

%%% Key points will be printed at top of second page
\begin{keypoints*}
\begin{itemize}
\item Evolving datasets are those that are often being expanded and improved. Users of evolving datasets require easy ways successive versions.
\item Using techniques adopted from software development, we present a workflow for maintaining and distributing versions of an evolving dataset. A new package \texttt{datastorr} enables fetching and loading successive versions directly in \texttt{R}.
\item Using semantic versioning to label versions of dataset conveys helps users identify the type of change that has occurred.
\end{itemize}
\end{keypoints*}

\newcommand{\smurl}[1]{\href{https://#1}{#1}}
\newcommand{\ghsmurl}[1]{\href{https://github.com/#1}{#1}}

\section{Introduction}

Publication of a quality dataset -- a collection of measurements, stored in one or several files -- is now considered a first-class scientific product. Increasingly, funding bodies, publishers, and scientific social norms are recognizing the value of sharing datasets, including as standalone products without any accompanying analyses \cite{Whitlock-2011,Fairbairn-2011,Piwowar-2011,VanNoorden-2013,Gibney-2013}. Evidence of this trend is seen in the increasing numbers of standalone ``Data papers'' appearing in both standard domain-level journals and specialised data journals. Yet, while the last decade has witnessed a rapid and exciting change in attitudes towards data sharing and publishing, the scientific community is still grappling with how to effectively disseminate and manage open-source datasets \cite{Whitlock-2011, Goodman-2014, Force11-2014, Lowndes-2017, Perkel-2016, VanNoorden-2013, Kratz-2015,Wilkinson-2016, Yenni-2018}. In particular, in some areas, such as our own area of ecology and evolution, we are only starting to support the fact that many high-quality datasets used in research should be evolving entities \cite{Yenni-2018}.

An evolving (or `living') dataset is one that is subject to recurrent or occasional change.  Typical changes may include improving the quality of existing data, adding new data, re-structuring the dataset content, or integrating with other datasets. For example, a dataset on biological organisms might be expanded through the addition of new records or improved through the correction of spelling mistakes in taxonomic names. In some cases, datasets may be expected to continue to evolve over extended periods \cite[e.g.][]{Ernest-2018}. Evolving datasets are never ``finished'', and as such there is no ``master'' or ``canonical'' version. Rather, as research around a data product grows, there might be many valid versions produced. Even datasets that are not initially envisioned as evolving, may become so as minor errors are identified and corrected during use. In either case, the most recent version of the dataset will typically contain the best-available information, but there are many reasons to go back to previous versions: to replicate previous analyses or to work on a stable version for downstream analyses or visualisation.  

A common approach taken by those maintaining an evolving dataset is to release sequential versions of the dataset, each containing a snapshot of the dataset at the time of release \cite[e.g.][]{Falster-2015, Pennell-2015a, Yenni-2018, Abolfathi-2018}. Ideally, the latest versions of an evolving dataset would be immediately available to all users across the globe, along with notes describing the changes when compared to previous versions. For the sake of reproducibility, previous versions of the dataset should be archived and remain available. In the recent past, small research groups have solved the issue of versioning data internally and informally, for example by emailing around the latest version. However, as science grows and moves towards more systematic publishing of data, scalable solutions are needed to distribute dataset versions to a wider variety of users.

One approach taken by large research consortia has been to create dedicated web servers for archiving and delivering of data. Projects such as the Sloan Digital Sky Survey (\smurl{https://www.sdss.org}) have sophisticated infrastructure and processes for storing and distributing successive versions of very large datasets \cite{Abolfathi-2018}. The issue of updating and expanding published data has also been addressed in some centralised repositories, like genetic sequences (via \texttt{GenBank}), where new data can be added and there exist abilities to correct errors in existing records. Yet these web databases require a level of funding and technological infrastructure that is beyond most research groups.

The vast majority of research projects are smaller and these currently rely on more generic data repositories for publishing of data. A common approach for publishing a dataset is to release it under a Digital Object Identifier (DOI) in a standalone data repository, such as DataDryad, Figshare, and Zenodo. While these platforms did not initially support versioning of datasets, all have subsequently added the ability to support multiple versions of a dataset, either under a single or different DOIs \cite{Nielsen-2017}. Yet, while these new features in principle allow users to access multiple versions of a dataset, the release, discovery, and access to multiple versions of a dataset is not always straightforward.

We believe more can be done to streamline the delivery of a potentially large number of dataset versions to users, especially for small research teams with limited budgets. There are at least three challenges. First, dataset developers need a cheap -- ideally free -- and reliable system  to create and distribute versions of an evolving datasets with low technical overhead. Second, users need an easy mechanism to discover the existence of new (or all) versions of an evolving dataset. Third, users need a mechanism to retrieve specific versions. For those using a computational language such as \texttt{R} \cite{R-2017}, all versions of an evolving dataset would ideally be both accessible and discoverable directly from within \texttt{R}.

In this article we outline how emerging technologies from software development  (Table \ref{tab:technologies}) can be used to address these challenges, enabling small research groups to create and maintain a stream of versions for small-to-medium sized datasets (up to 2Gb), and distribute these directly into the \texttt{R} computational environment for a potentially unlimited number of users at zero financial cost and minimal technical overhead. To achieve this we developed a new \texttt{R} package called \texttt{datastorr}, which together with other the technologies allows for easy and scalable delivery of successive versions of an evolving dataset directly into \texttt{R}. At time of publishing this article, this workflow was being used to distribute versions of datasets across a wide range of topics (Table \ref{tab:examples}), suggesting a potentially wide domain of application.


\begin{table*}[ht!]
\centering
\caption{Overview of technologies used to maintain, store, and distribute versions of an evolving dataset as described in this paper.}
\vspace{0.2cm}
  \begin{tabular}{p{3cm}p{13cm}}
  \hline
  \textbf{Technology} & \textbf{Description} \\\hline
  % \textbf{Technologies} &\\
  \texttt{git} & Open source version control system used for tracking progressive changes in a set of text files, typically computer code but also data.\\
  \texttt{GitHub} & A commercial web platform available at \smurl{github.com} for sharing, visualising, and managing \texttt{git} repositories. Includes ability to browse the `history', `issue' tracking, and ability to host `releases'. Also has a well developed Application Programming Interface (API) enabling programmatic access to dataset releases.\\ 
  \texttt{R} & Widely-used and open-source language for data processing and statistical analysis.\\
  \texttt{datastorr} & A package in \texttt{R} used to fetch releases of an evolving dataset hosted on \texttt{GitHub}. \\
  \texttt{semantic versioning} & The process of assigning unique version numbers in a particular format to successive versions of a digital product; traditionally applied to software but here to an evolving dataset.\\
  \hline
  \end{tabular}
\label{tab:technologies}
\end{table*}

\begin{figure*}[!hb]
\centering
\includegraphics[width=\linewidth]{figures/Figure-stack.pdf}
\caption{Overview of the workflow, different parties and technologies involved in maintaining an distributing versions of an evolving dataset via \texttt{datastorr}.  Core features of our approach are shown with black boxes and arrows. Optional extensions are shown in grey (see Discussion for details).}
\label{fig:technology_stack}
\end{figure*}

\section{A lightweight, cheap, and scalable workflow for delivering versions of an evolving dataset into \texttt{R}}

In brief, the workflow we present here borrows best practices for software development \cite{Perez-Riverol-2016} and applies them to the challenge of maintaining and distributing versions of an evolving dataset. Our approach envisions multiple parties involved in the creation and/or use of a versioned dataset, including \emph{Developers}, \emph{Contributors} and \emph{Users} (Fig. \ref{fig:technology_stack}). Each of these will likely have different goals and requirements (see Table \ref{tab:user_requirements}). When building a piece of software, Developers maintain a core set of code which produces the binary executable file that is eventually installed on a User's local computer. Analogously, Developers of an evolving dataset maintain a core set of files (the ``code''), which produces an organised dataset that can be ``installed'' (i.e., loaded) on a User's local computer. In either the development of software or data, successive versions -- called ``releases'' -- are distributed as snapshots of the generated product at a particular point in time. 

The similarity in workflow between software and data allows us to deploy the re-purpose some of the same technological platforms that are used to maintain and distribute versions of a software product to maintain and distribute versions of an evolving dataset (Table \ref{tab:technologies}). Importantly, these tools are available free of charge for open source projects and already well developed -- ensuring high-level performance and stability. Moreover, the combination of technologies allow us to address the goals and requirements of the different parties involved in the creation and use of a versioned dataset (see Table \ref{tab:user_requirements}). 

An overview of the proposed system is as follows.
\begin{itemize}
  \item Raw data files are stored under version control in a \texttt{git} repository --  a free and leading version control system used in software development -- by the dataset Developers. All the files that go together to build a single dataset are stored in the repository, together with any code used to manipulate these files to create the dataset that is ultimately distributed.
  \item Changes to the raw data files and code are tracked by the Developers using  \texttt{git}'s ability to make ``commits'' -- granular and annotated snapshots of the source files over time.
  \item The \texttt{git} repository is hosted on \texttt{GitHub} -- a leading platform for hosting, enabling multiple Developers or other Contributors to work collaboratively on improving a dataset (Fig. \ref{fig:technology_stack}).
  \item Developers use the files in the repository to make a release of the dataset -- a snapshot of the generated data product at a particular commit -- and upload these to \texttt{GitHub}, where they are hosted alongside the raw files and (optionally) labelled using ``semantic versioning''. The version labels indicate both the ordering of versions and the magnitude of change expected between different versions.
  \item Using the \texttt{datastorr} package, Users can both retrieve a list of all available versions of the dataset, and retrieve particular versions of the dataset on demand, and load them directly into \texttt{R}. 
\end{itemize}

Below we elaborate on each of the different technologies.

\subsection{Version control}

Version control, primarily an open-source variety called \texttt{git}, has become widespread in software development. In practice, version control tracks line-by-line changes in text files and creates and maintains a history of those changes. Increasingly version control has been applied to scientific code and also data management, especially for small-to-medium sized datasets \cite{Ram-2013, Perkel-2016, Lowndes-2017}. \texttt{git} is attractive for data management because it tracks all changes in monitored files, provided these are saved in text format (e.g., ``.csv'', ``.tsv'', ``.txt''; with some tricks git can also indicate changes in some other file types such as ``.xlsx''). It  allows users to annotate commits with informative messages detailing the rationale for those changes.  The ``history'' of commits is also visible to anyone interacting with the repository. In its present form, \texttt{git} can handle individual data files at least up to 100MB, which includes a large fraction of scientific cases.

As a general strategy for tracking a dataset under version control with \texttt{git}, we recommend:
\begin{itemize}
  \item Developers establish a separate \texttt{git} repository for each dataset to be distributed.
  \item Saving all files as plain text, so that \texttt{git} can identify line-by-line changes. For example, save tabular data as a ``csv''.
  \item Saving data in their rawest form. In some datasets you might only have a single file. Others may have may files that get manipulated or combined in some way to produce a unified product.
  \item Including in the \texttt{git} repository any code needed to manipulate or compile the raw data files into the final dataset. For example, you might combine many independent datasets into one unified dataset.
  \item Documenting any changes in the dataset by making a commit in the \texttt{git} repository, with informative message outlining why the change was made.
\end{itemize}


\subsection{Hosting and distributing versions of an evolving dataset}

Datasets stored under version control via \texttt{git} reach their real potential when hosted at a suitable internet hosting service \cite{Ram-2013,Perkel-2016}. Here we focus on the platform \texttt{GitHub} (Table \ref{tab:technologies}). Hosting of a \texttt{git} repository enables dataset Developers to connect with other potential Contributors and also Users (Fig. \ref{fig:technology_stack}). These platforms are designed to work with \texttt{git} repositories, and thus offer many helpful features, such as ability to record issues, host documentation, or review edits over time.

Of particular interest for current purposes, is \texttt{GitHub}'s ability to host a stream of releases from the dataset, alongside the \texttt{git} repository containing all the raw files. Each release is linked to a specific commit in the \texttt{git} repository history and occur at points where the dataset Developer decided to generate a new version of the data for distribution. While Users could in principle download the entire \texttt{git} repository, actually all they want (most of the time) are the releases.

Deciding when to make a new release is at the discretion of the dataset Developer. In practice, one makes fewer releases than one does commits into the \texttt{git} repository, though there is nothing stopping Developers from releasing a new version for every commit. The flexibility here allows Developers to do internal work between releases and only release the data to Users when the revision represents a clear improvement on the previous release.

Another important consideration is that websites like \texttt{GitHub} naturally cater for two types of data Users accessing the data: those that interact with the data via point and click downloading, and those that use programmatic interaction (Fig. \ref{fig:technology_stack}, Table \ref{tab:user_requirements}). Specifically, \texttt{GitHub} releases can be downloaded directly by Users or accessed programmatically via the \texttt{GitHub} API.

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{figures/Figure-versions.pdf}
\caption{
% could use a more descriptive legend. Maybe walk the reader through it a bit more. Also, I didnâ€™t see (a) and (b) in that figure.
Semantic versioning allows dataset Developers to communicate to Users the types of changes that have occurred between successive versions of an evolving dataset.}
\label{fig:semantic}
\end{figure}


\begin{table*}[b!]
\centering
\caption{Example datasets currently delivered using the \texttt{datastorr} package for \texttt{R}}
\vspace{0.2cm}
  \begin{tabular}{p{4cm}p{11cm}}
  \hline
   \textbf{\texttt{GitHub} repository} & \textbf{Dataset description} \\ \hline
  \ghsmurl{dfalster/baad.data} & Size dimensions of plants for many species from across the world \cite{Falster-2015}.\\
  \ghsmurl{traitecoevo/taxonlookup} & Taxonomy of world's land plants \cite{Pennell-2015a}.\\
  \ghsmurl{ecohealthalliance/cites} & Trade details from Convention on International Trade in Endangered Species (CITES)\\
  \ghsmurl{madams1/nbadata} & Statistics from the National Basketball Association (NBA) seasons 1996-97 to 2016-17 \\
  \ghsmurl{madams1/floridainmates} & Dataset from the Florida Department of Corrections on the state's inmate population\\
  \ghsmurl{traitecoevo/fungaltraits} & Dataset on the traits of fungi \cite{Cornwell-2018}.\\

  \hline
  \end{tabular}
\label{tab:examples}
\end{table*}

\subsection{Semantic versioning}

To realize the full benefits of a versioned controlled dataset, Users should be able to easily intuit the types of changes that have occurred among versions. Since software development has effectively already dealt with almost identical; problem in the labelling of software releases, we suggest there is benefit in adopting the best-practices from that field.  

Specifically, we suggest applying the theory of semantic versioning, developed for software distribution (\href{http://semver.org/}{semver.org}), to successive releases of an evolving dataset (Fig \ref{fig:semantic}). Semantic versioning uses a tri-digit notation of the form ``X.Y.Z'' for successive versions, where X, Y, and Z are non-negative integers. For example, version ``1.0.0''. Changes in the version number then signal the type of changes that occurred in the dataset (Fig \ref{fig:semantic}). Applying semantic versioning to data, a change from:
\begin{itemize}
  \item {\bf 1.0.0 $\rightarrow$ 1.0.1}: implies a ``correction'' (or ``patch''), for example a small error correction which is unlikely to break, or change in a substantial way, a user's analyses, although there may be minor changes in the results.
  \item {\bf 1.0.0 $\rightarrow$ 1.1.0}: implies a ``minor enhancement'', for example adding a new study to a meta-analysis dataset (while otherwise maintaining the same dataset structure); this change is large enough that in the view of the dataset Developer it might change the results of an analysis done with the prior version and so warrants closer inspection.
  \item {\bf 1.0.0 $\rightarrow$ 2.0.0}: implies a ``major update'', for example improving the entire structure of the dataset, or adding of new columns. These changes are likely to change the results of most user's analyses or to break code that was written to work on the previous version of the dataset.
\end{itemize}
Any user of an evolving dataset where releases were tagged using semantic versioning, would immediately know the Developer's view of the types of the magnitude of the changes between versions.  Note that as with software this taxonomy of changes is a language to more nuanced way to communicate from the Developer to the User, rather than a set of fixed rules. Because users of data--like users of software--do unpredictable things, the Developer may classify a change as minor, but still could be break some downstream analyses.  

\subsection{Loading data versions directly into \texttt{R} using the \texttt{datastorr} package}

For efficient usage and to aid reproducibility, many users will want access to all versions of any particular dataset programmatically (Table \ref{tab:user_requirements}). Code to access a stream of \texttt{GitHub} releases could be written individually by each user, but this creates an unnecessary technological hurdle. To make it easier for users to access versioned data via code, we developed a new package for the \texttt{R} platform, as one of the most prominent platforms for data science \cite{R-2017}.

Our package, called \texttt{datastorr} (\smurl{github.com/ropenscilabs/datastorr}), facilitates access to releases of any evolving dataset hosted on \texttt{GitHub} (Fig. \ref{fig:technology_stack}). Specifically, the \texttt{datastorr} package: 1) Contains the main code needed to interact with the \texttt{GitHub} API to retrieve versions of the dataset; and 2) Enables users to construct the shell of a second, dataset-specific \texttt{R} package, which can be distributed and used to access releases for a specific repository stored on \texttt{GitHub}. Using \texttt{datastorr}, a researcher can create and distribute a custom \texttt{R} package that facilitates access to their data with (very) minimal computational skills.

For example, \texttt{datastorr} has been used to build several packages (Table \ref{tab:examples}), including \texttt{baad.data} (\smurl{github.com/traitecoevo/baad.data}), which is an interface to the Biomass and Allometry Database \cite{Falster-2015} stored at \smurl{github.com/dfalster/baad}. The R package \texttt{baad.data} consists of only a few simple functions and associated help files, that were automatically generated with \texttt{datastorr}. For a user, accessing a version of the data is a simple as typing a single line of code (Fig. \ref{fig:technology_stack}). Accessing a different version of the data involves changing only the version number. From the user's perspective, the existence of the \texttt{baad.data} and \texttt{datastorr} packages makes reproducing analyses using specific versions of the data possible [e.g.][]\cite{Duursma-2016,Falster-2018}.


Using \texttt{datastorr}, dataset Developers can set up their own \texttt{R} package to deliver versions of an evolving dataset simply by providing:
\begin{enumerate}
  \item a \texttt{GitHub} repository name (e.g., ``traitecoevo/baad.data'') where releases are stored;
  \item the filename in the release that contains data;
  \item the function used to load the data file into \texttt{R}.
\end{enumerate}

Then as the dataset grows over time, the Developers update the \texttt{git} repository and create a \texttt{GitHub} release with a new version number. All the releases are simultaneously available to any user, both point-and-click and programmatically.

The dataset-specific packages created by \texttt{datastorr} are designed to be computationally efficient and also work offline. Packages created by \texttt{datastorr} contain no actual data, only the rules for fetching the data. As such, the basic package structure is quick to install and takes up virtually no space on the user's hard-drive. The package functions by fetching each data version once (the first time it is requested), and then caching these files locally for future reuse. Moreover, users can store several versions of an evolving dataset on their computer and unambiguously access different versions with single function.

\begin{table*}[t!]
\centering
\caption{Goals and requirements of different parties involved in creating and using and evolving dataset.}
\vspace{0.2cm}
  \begin{tabular}{p{3cm}p{5cm}p{8cm}}
  \hline
% Latex being the worst has made the overflow in the second column flow through to affect the third. I used to know how to deal with this...
  \textbf{Group} & \textbf{Primary goal} & \textbf{Requirements} \\ \hline
  Developer & Create and distribute versions of an evolving dataset & Low technical overhead \\
    & & Low initial and ongoing cost and maintenance \\
    & & Easy workflow for releasing new versions \\
    & & Enable User feedback in error checking and contributions \\
    & & Long term preservation \\
  Contributor & Contribute to future versions of an evolving dataset & Add new data \\
    & & Report errors in existing data \\
  Users (all) & Easy access to all versions of an evolving dataset & Access metadata and background information\\
    & & Access to all versions of a dataset\\
    & & Ability to give feedback and contribute \\
    & & Long term stability \\
  Users (programmatic) &  As above, plus\\
    & & Programmatic access to all versions of an evolving dataset \\
    & & Reproduce products using specific versions of an evolving dataset \\
    & & Easy installation \\
  \hline
  \end{tabular}
\label{tab:user_requirements}
\end{table*}

\section{Discussion}

The key issue we are dealing with in this article may be familiar to many readers: many datasets are constantly evolving and, despite tremendous advances in data sharing and associated technologies over the last decade, there is as yet little consensus about how to maintain and distribute multiple versions of an evolving dataset, especially for small research teams. While such teams could in principle create their own dynamic web interface, the technological hurdles, cost and maintenance required are discouraging. Moreover, existing data sharing platforms offer a limited set of features for the delivery of successive versions of an evolving dataset. This suggests there is a substantial need for an easy, cheap, and scalable solution for maintaining and distributing successive versions of an evolving dataset. By adopting open-source and scalable practices from software development, we believe a workable system already largely exists. To aid this process, we created the \texttt{datastorr} package to deliver dataset versions directly into the \texttt{R} environment. The approach and package is already being used to deliver versions of several evolving datasets spanning a wide range of topics (Table \ref{tab:examples}). Moreover, as it builds off established and open source software and data science platforms (Table \ref{tab:technologies}), the proposed system is already easy to deploy on relatively large scale.

\subsection{Towards an ecosystem for evolving data}

Our contribution here connects with a growing number of recommendations and technologies supporting the publishing and reuse of evolving data. Such contributions include community guidance on good practice in data curation \cite{Goodman-2014, Lowndes-2017}, data citation \cite{Force11-2014} and the FAIR principles for making datasets Findable, Accessible, Interoperable, and Reusable by both machines and humans \cite{Wilkinson-2016}. In our proposed system, information about appropriate attribution for any dataset  (whatever that is determined to be) should be made readily available, either on the landing page within GitHub, or even better distributed as part of the versioned dataset itself. Similarly, datasets can be structured to make them follow the FAIR principles, to the extent possible. Notably, our workflow with the \texttt{datastorr} package demands machine access to datasets - a core focus for the FAIR principles. While our proposed workflow does not currently enhance discoverability of new datasets, this is a broad challenge faced by all data platforms and researchers.

While our package \texttt{datastorr} offers an easy way for users of the \texttt{R} ecosystem to directly access dataset version, users of other languages can also access the datasets. Moreover, packages similar to \texttt{datastorr} would ideally be developed to make accessing dataset versions as easy as it is with \texttt{datastorr}.

Within the \texttt{R} ecosystem, the \texttt{datastorr} package complements other approaches for creating and delivering datasets. One common approach used within \texttt{R} is to embed data directly within an \texttt{R} package, which can then be distributed via the Comprehensive R Archive Network (\smurl{cran.r-project.org}). Moreover, dedicated packages are being developed to assist dataset Developers in creating data packages \cite{Finak-2018}. An advantage of this approach, compared to ours, is that the data are immediately available in the package (whereas our packages only contain instructions for fetching the data). This advantage however also brings limitations. Notably, datasets must be under 5MB, and only one version of a dataset package can be installed on any given machine at any one time.  \texttt{datastorr} offers a viable approach for overcoming these limitations.

There are also many emerging or alternative technologies that offer other possible ways to implement a system for storing and distributing versions of an evolving dataset. Our solution currently emphasises the platform GitHub, but similar functions could be achieved via other \texttt{git} hosts such as \href{http://bitbucket.org}{bitbucket.org} and \href{http://gitlab.com}{gitlab.com}. Git repositories can also be extended to accommodate larger files using  features like \href{https://git-lfs.github.com/}{Git-Large File Storage} or \href{http://git-annex.branchable.com/}{git-annex}. More fundamentally, there are  emerging alternatives for version control specifically designed for data, such as the \smurl{datproject.org}, and other new platforms for distributing data, such as the \href{https://en.wikipedia.org/wiki/CKAN}{Comprehensive Knowledge Archive Network, (CKAN)} and \href{https://okfn.org/}{Open Knowledge International, (OKFN)}. 

The key here is not the specific technology, but rather the concept of creating, maintaining, and distributing versions of an evolving dataset, which may be achieved with all of these approaches. Indeed, as with every technology the best available approach is certain to evolve, especially as emerging technologies facilitate even better delivery of data in the future.

\subsection{Further advantages and extensions}

A central feature of the proposed system is that data are maintained in the cloud. This has two main benefits: first, it provides a platform for multiple data contributors to sync their files and correspond about changes in the dataset, and second, it allows for hosting of a stream of data releases for distribution (Fig. \ref{fig:technology_stack}). Cloud systems thus act as a central point for the collection, curation, and distribution of the data. Additionally, one of the greatest benefits of using cloud-based tools like  \texttt{GitHub} for development of software and data has been the way they encourage contributions from multiple individuals working simultaneously --- including from people from outside the initial group of project participants \cite{Rogers-2013, Perkel-2016}. Multiple Developers can make changes to different parts of the code (or, in our case, data) and the \texttt{git} system will integrate these together or, when needed, flag where there are conflicts that need to be resolved. The proposed system of data delivery thus has the added benefit of facilitating seamless and transparent collaboration among research groups in the construction and maintenance of datasets.

An important concern for any data delivery system is the stability and reliability of the system. In the short term, users want minimal downtime, high speed, and seamless operation. As one of the largest companies hosting computer source code, \texttt{GitHub} provides exceptional performance in this regard -- certainly as good or better than nearly any system scientists might build themselves. Thus, short-term concerns of reliable and fast performance are almost guaranteed.

In the long term, scientists want their datasets, software, and papers to preserved and remain accessible. While our proposed system for data delivery does not guarantee long-term preservation, users can also choose to automatically archive data-versions released on \texttt{GitHub} version in one of several traditional data archives, with a DOI (Digital Object Identifier) minted for each release. Currently, both \texttt{Zenodo} and  \texttt{FigShare} each integrate with \texttt{GitHub} for archiving of material hosted there. Ideally, tools like \texttt{datastorr} would also be developed to pull versions from these archives too. 

\section{Availability of source code and requirements}

\begin{itemize}
\item Project name:  \texttt{datastorr} 
\item Project home page: \smurl{github.com/ropenscilabs/datastorr}.
\item Operating system(s): Platform independent
\item Programming language: R
\item License: MIT
\end{itemize}

\section{Declarations}

\subsection{List of abbreviations}
API = An Application Programming Interface provides a set of protocols for exchanging information. DOI = Digital Object Identifier.

\subsection{Ethical Approval}
Not applicable.

\subsection{Consent for publication}
Not applicable.
\subsection{Competing Interests}
The author(s) declare that they have no competing interests. 

\subsection{Funding}

DSF was funded by the Australian Research Council. MWP was funded by a NSERC Discovery Grant.

\subsection{Author's Contributions}
RF developed the datastorr package. All authors discussed the concepts and wrote the paper. 

\section{Acknowledgements}

We thank D Noble for comments on an earlier draft and C Boettiger for helpful discussions. 

% \section{Authors' information}

% You may choose to use this section to include any relevant information about the author(s) that may aid the reader's interpretation of the article, and understand the standpoint of the author(s). This may include details about the authors' qualifications, current positions they hold at institutions or societies, or any other relevant background information. Please refer to authors using their initials. Note this section should not be used to describe any competing interests.

\bibliography{refs}

\end{document}
